{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# **03 Baseline Model**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Objectives\n",
        "\n",
        "* Build a reproducible baseline machine learning pipeline using logistic regression for the intrusion detection dataset.\n",
        "\n",
        "## Inputs\n",
        "\n",
        "* `data/raw/cybersecurity_intrusion_data.csv`\n",
        "\n",
        "## Outputs\n",
        "\n",
        "* Baseline performance metrics printed within the notebook.\n",
        "* A fitted scikit-learn pipeline ready for downstream experimentation.\n",
        "\n",
        "## Additional Comments\n",
        "\n",
        "* The notebook adopts the shared template structure to stay consistent with the rest of the project.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Change working directory\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "* These notebooks live in the `jupyter_notebooks` subfolder, so the project root is one level up.\n",
        "* Capture the current directory so we can move to the repository root before referencing project-relative paths.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "current_dir = os.getcwd()\n",
        "current_dir"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "* Set the working directory to the project root so relative paths in the remaining cells resolve correctly.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "os.chdir(os.path.dirname(current_dir))\n",
        "print(f\"You set a new current directory: {os.getcwd()}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "* Double-check the update before continuing.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "current_dir = os.getcwd()\n",
        "current_dir"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Data Loading\n",
        "* Load the intrusion detection dataset and perform a quick sanity check on its structure.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "DATA_PATH = 'data/raw/cybersecurity_intrusion_data.csv'\n",
        "df = pd.read_csv(DATA_PATH)\n",
        "print(f'Dataset shape: {df.shape}')\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Feature Preparation\n",
        "* Remove identifier columns, separate the target, and catalogue numeric versus categorical predictors for preprocessing.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "target_col = 'attack_detected'\n",
        "id_cols = ['session_id']\n",
        "feature_frame = df.drop(columns=id_cols + [target_col])\n",
        "target = df[target_col]\n",
        "numeric_features = feature_frame.select_dtypes(include=['int64', 'float64']).columns.tolist()\n",
        "categorical_features = feature_frame.select_dtypes(include=['object']).columns.tolist()\n",
        "print('Numeric features:', numeric_features)\n",
        "print('Categorical features:', categorical_features)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Train/Test Split and Pipeline Definition\n",
        "* Split the data with stratification to keep class balance and define a preprocessing + logistic regression pipeline.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.pipeline import Pipeline\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    feature_frame,\n",
        "    target,\n",
        "    test_size=0.2,\n",
        "    random_state=42,\n",
        "    stratify=target\n",
        ")\n",
        "\n",
        "preprocessor = ColumnTransformer(\n",
        "    transformers=[\n",
        "        ('num', StandardScaler(), numeric_features),\n",
        "        ('cat', OneHotEncoder(handle_unknown='ignore'), categorical_features)\n",
        "    ]\n",
        ")\n",
        "\n",
        "baseline_pipeline = Pipeline(\n",
        "    steps=[\n",
        "        ('preprocessor', preprocessor),\n",
        "        ('classifier', LogisticRegression(max_iter=1000))\n",
        "    ]\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Model Training and Evaluation\n",
        "* Fit the pipeline and capture straightforward metrics to benchmark future experiments.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "\n",
        "baseline_pipeline.fit(X_train, y_train)\n",
        "y_pred = baseline_pipeline.predict(X_test)\n",
        "\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f'Baseline accuracy: {accuracy:.3f}')\n",
        "print('Classification report:')\n",
        "print(classification_report(y_test, y_pred))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Conclusions and Next Steps\n",
        "* Logistic regression establishes a ~baseline accuracy that will anchor future model comparisons.\n",
        "* Consider exploring feature interactions or more expressive models (e.g., tree ensembles) to improve recall for the attack class.\n",
        "* Persist the fitted pipeline to disk once its performance is validated against additional evaluation metrics.\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Data Practitioner Jupyter Notebook.ipynb",
      "provenance": [],
      "toc_visible": true
    },
    "interpreter": {
      "hash": "8b8334dab9339717f727a1deaf837b322d7a41c20d15cc86be99a8e69ceec8ce"
    },
    "kernelspec": {
      "display_name": "Python 3.8.12 64-bit ('3.8.12': pyenv)",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.12 (default, Nov  7 2022, 16:45:55) \n[GCC 9.4.0]"
    },
    "orig_nbformat": 2
  },
  "nbformat": 4,
  "nbformat_minor": 2
}